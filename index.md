---
layout: default
title: Relational Representation Learning
---

## <a name="overview"></a> Overview

**Date and Time:** 8:45 AM - 6:00 PM, December 8, 2018  
**Location:** Room 517A, Palais des Congrès de Montréal, Montréal, Canada

Relational reasoning, *i.e.*, learning and inference with relational data, is key to understanding how objects interact with each other and give rise to complex phenomena in the everyday world. Well-known applications include knowledge base completion and social network analysis. Although many relational datasets are available, integrating them directly into modern machine learning algorithms and systems that rely on continuous, gradient-based optimization and make strong i.i.d. assumptions is challenging. Relational representation learning has the potential to overcome these obstacles: it enables the fusion of recent advancements like deep learning and relational reasoning to learn from high-dimensional data. Success of such methods can facilitate novel applications of relational reasoning in areas like scene understanding, visual question-answering, chemical and biological processes, code analysis, and decision-making in multi-agent systems.

How should we rethink classical representation learning theory for relational representations? Classical approaches based on dimensionality reduction techniques such as isoMap and spectral decompositions still serve as strong baselines and are slowly paving the way for modern methods in relational representation learning based on random walks over graphs, message-passing in neural networks, group-invariant deep architectures etc. amongst many others. How can systems be designed and potentially deployed for large scale representation learning? What are promising avenues, beyond traditional applications like knowledge base and social network analysis, that can benefit from relational representation learning?

This workshop aims to bring together researchers from both academia and industry interested in addressing various aspects of representation learning for relational reasoning. Topics include, but are not limited to:

* Algorithmic approaches: embedding methods, probabilistic approaches based on latent variable modeling, message-passing neural networks, dimensionality reduction techniques etc. for relational data
* How to evaluate relational representations in terms of trade-offs with different learning objectives?
* Theoretical aspects: when and why do learned representations aid relational reasoning? How does the non-i.i.d. nature of relational data conflict with our current understanding of representation learning?
* Optimization and scalability challenges due to the inherent discreteness and curse of dimensionality of relational datasets and challenges in negative sampling
* Domain-specific applications of relational representation learning in areas like graph mining, natural language processing, reinforcement learning, programming language analysis etc.
* Any other topic of interest for relational representation learning


## <a name="speakers"></a> Invited Speakers
[Joan Bruna](https://cims.nyu.edu/~bruna/), New York University    
[Pedro Domingos](https://homes.cs.washington.edu/~pedrod/), University of Washington   
[Lise Getoor](https://getoor.soe.ucsc.edu/home), UC Santa Cruz   
[Timothy Lillicrap](http://contrastiveconvergence.net/~timothylillicrap/index.php), Google Deepmind     
[Marina Meila](https://www.stat.washington.edu/mmp/), University of Washington   
[Maximilian Nickel](https://mnick.github.io/), Facebook Artificial Intelligence Research     
<!-- [Oriol Vinyals](https://ai.google/research/people/OriolVinyals), Google Deepmind    
More TBA!  -->

## <a name="schedule"></a> Schedule
Coming Soon!

## <a name="submission"></a> Call for Papers
### Paper Submission Instructions
Workshop papers should be at most **4 pages of content**, including text and figures. Additional pages containing only bibliographic references can be included without penalty. 

Authors will not be penalized for including an appendix of supplementary material after the references. However, reviewers will not be required to consult any appendices to make their decisions. The main 4-page paper should adequately describe the work and its contributions.

Papers should be anonymized and adhere to the NeurIPS conference format: [https://neurips.cc/Conferences/2018/PaperInformation/StyleFiles](https://neurips.cc/Conferences/2018/PaperInformation/StyleFiles)

**Submission Site:** [https://cmt3.research.microsoft.com/R2L2018](https://cmt3.research.microsoft.com/R2L2018)

### Peer Review and Acceptance Criteria
All submissions will go through a double-blind per review process. Accepted papers will be chosen based on techincal merit, interest, and novelty. 

All accepted papers will be included in one of the two poster presentation and lightning talk sessions on the day of the workshop. Some accepted papers will be invited to give a contributed oral talks. 

### Important Dates 
* **Submission Deadline: October 19th, 2018, 23:59 PST**
* Notification of Acceptance: November 2nd, 2018
* Camera-ready Due: November 16th, 2018
* Workshop: Decmeber 8, 2018

## Organizers <a name="organizers"></a> 
[Aditya Grover](http://aditya-grover.github.io/), Stanford University  
[Paroma Varma](https://paroma.github.io/), Stanford University   
[Fred Sala](https://stanford.edu/~fredsala/), Stanford University  
[Steven Holtzen](https://web.cs.ucla.edu/~sholtzen/), University of California, Los Angeles  
[Jennifer Neville](https://www.cs.purdue.edu/homes/neville/index.html), Purdue University  
[Stefano Ermon](https://cs.stanford.edu/~ermon/), Stanford University  
[Christopher Ré](https://cs.stanford.edu/people/chrismre/), Stanford University
